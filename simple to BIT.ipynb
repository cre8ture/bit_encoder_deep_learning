{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMs/wBcrL4kcYRjL9fpraon"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"DgAG6QzVOC10","executionInfo":{"status":"ok","timestamp":1690828375922,"user_tz":240,"elapsed":8,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"outputs":[],"source":["# https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0#:~:text=In%20this%20tutorial%20we%20will%20implement%20a%20simple,allow%20you%20to%20build%20your%20customized%20neural%20networks."]},{"cell_type":"markdown","source":["# we transform a simple model archtecture and use bit encoding"],"metadata":{"id":"j_sAegXtRd-c"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn"],"metadata":{"id":"VnbsATPaOhI0","executionInfo":{"status":"ok","timestamp":1690828387719,"user_tz":240,"elapsed":11803,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# sample data\n","X = torch.tensor(([2, 9], [1, 5], [3, 6]), dtype=torch.float) # 3 X 2 tensor\n","y = torch.tensor(([92], [100], [89]), dtype=torch.float) # 3 X 1 tensor\n","xPredicted = torch.tensor(([4, 8]), dtype=torch.float) # 1 X 2 tensor"],"metadata":{"id":"tJYHYG7GOnoP","executionInfo":{"status":"ok","timestamp":1690828387721,"user_tz":240,"elapsed":43,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["print(X.size())\n","print(y.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZYVL4uRDO6r3","executionInfo":{"status":"ok","timestamp":1690828387722,"user_tz":240,"elapsed":37,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}},"outputId":"ab08f0f5-91df-4ffb-9ae9-93c8179bbfe2"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 2])\n","torch.Size([3, 1])\n"]}]},{"cell_type":"code","source":["# scale units\n","X_max, _ = torch.max(X, 0)\n","xPredicted_max, _ = torch.max(xPredicted, 0)\n","\n","X = torch.div(X, X_max)\n","xPredicted = torch.div(xPredicted, xPredicted_max)\n","y = y / 100  # max test score is 100"],"metadata":{"id":"OXu0sGA7P-mU","executionInfo":{"status":"ok","timestamp":1690828387956,"user_tz":240,"elapsed":243,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# import torch\n","# T_data = [[[1., 2., 3.], [4., 5., 6.]], [[7., 8., 9.], [10., 11., 12.]], [[13., 14., 15.], [16., 17., 18.]]]\n","# T = torch.tensor(T_data, dtype=torch.float)\n","# print(T.shape)"],"metadata":{"id":"5w2yQVQqQjcV","executionInfo":{"status":"ok","timestamp":1690828387957,"user_tz":240,"elapsed":7,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X_mean = torch.mean(X, dim=0, keepdim=True)\n","X_std = torch.std(X, dim=0, keepdim=True)\n","X_normalized = (X - X_mean) / X_std\n"],"metadata":{"id":"DpzwXpp4e4XI","executionInfo":{"status":"ok","timestamp":1690828387958,"user_tz":240,"elapsed":8,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["xPredicted_normalized = (xPredicted - X_mean) / X_std\n"],"metadata":{"id":"VcvDljxie6ZB","executionInfo":{"status":"ok","timestamp":1690828387959,"user_tz":240,"elapsed":8,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Neural_Network(nn.Module):\n","    def __init__(self, learning_rate=0.001):\n","        super(Neural_Network, self).__init__()\n","        # parameters\n","        self.inputSize = 2\n","        self.outputSize = 1\n","        self.hiddenSize = 3\n","        self.learning_rate = learning_rate\n","\n","        self.relu = nn.ReLU()\n","\n","        # weights\n","        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 2 X 3 tensor\n","        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n","        self.bias1 = torch.randn(self.hiddenSize) # 3 tensor\n","        self.bias2 = torch.randn(self.outputSize) # 1 tensor\n","\n","    def forward(self, X):\n","        self.z = torch.matmul(X, self.W1) + self.bias1 # broadcast addition\n","        self.z2 = self.leaky_relu(self.z) # activation function\n","        self.z3 = torch.matmul(self.z2, self.W2) + self.bias2 # broadcast addition\n","        o = self.leaky_relu(self.z3) # final activation function\n","        return o\n","\n","    # def sigmoid(self, s):\n","    #     return 1 / (1 + torch.exp(-s))\n","\n","    # def sigmoidPrime(self, s):\n","    #     return s * (1 - s)\n","\n","    def leaky_relu(self, s, negative_slope=0.01):\n","      return torch.where(s >= 0, s, s * negative_slope)\n","\n","    def leaky_relu_prime(self, s, negative_slope=0.01):\n","      return torch.where(s >= 0, torch.ones_like(s), torch.full_like(s, negative_slope))\n","\n","\n","    def backward(self, X, y, o):\n","        self.o_error = y - o # error in output\n","        self.o_delta = self.o_error * self.leaky_relu_prime(self.z3) # applying derivative of sigmoid to error\n","        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n","        self.z2_delta = self.z2_error * self.leaky_relu_prime(self.z2)\n","\n","        dW1 = torch.matmul(torch.t(X), self.z2_delta)\n","        dW2 = torch.matmul(torch.t(self.z2), self.o_delta)\n","\n","        dbias1 = torch.sum(self.z2_delta, dim=0)\n","        dbias2 = torch.sum(self.o_delta, dim=0)\n","\n","        # update weights and biases with scaled gradients\n","        self.W1 += dW1 * self.learning_rate\n","        self.W2 += dW2 * self.learning_rate\n","        self.bias1 += dbias1 * self.learning_rate\n","        self.bias2 += dbias2 * self.learning_rate\n","\n","    def train(self, X, y):\n","        # forward + backward pass for training\n","        o = self.forward(X)\n","        self.backward(X, y, o)\n","\n","    def saveWeights(self, model):\n","        # we will use the PyTorch internal storage functions\n","        torch.save(model, \"NN\")\n","\n","    def predict(self):\n","        print (\"Predicted data based on trained weights: \")\n","        print (\"Input (scaled): \\n\" + str(xPredicted))\n","        print (\"Output: \\n\" + str(self.forward(xPredicted)))\n"],"metadata":{"id":"x1aevp1oQjUP","executionInfo":{"status":"ok","timestamp":1690828387960,"user_tz":240,"elapsed":8,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Training\n","# loss begins to increase!\n","NN = Neural_Network()\n","for i in range(1000):  # trains the NN 10,000 times\n","    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X_normalized))**2).detach().item()))  # mean sum squared loss\n","    NN.train(X_normalized, y)\n","NN.saveWeights(NN)\n","\n","# Predict using normalized data\n","print (\"Predicted data based on trained weights: \")\n","print (\"Input (scaled): \\n\" + str(xPredicted_normalized))\n","print (\"Output: \\n\" + str(NN(xPredicted_normalized)))"],"metadata":{"id":"vGC-_ToZUOOM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we apply early stopping\n","NN = Neural_Network()\n","min_val_loss = float('inf')\n","patience = 10  # Number of epochs to wait before stopping training if validation loss does not decrease\n","wait = 0\n","\n","for i in range(1000):\n","    # Training\n","    train_loss = torch.mean((y - NN(X_normalized))**2).detach().item()\n","    print(\"#\" + str(i) + \" Train Loss: \" + str(train_loss))\n","    NN.train(X_normalized, y)\n","\n","    # Validation\n","    # val_loss = ...  # Calculate validation loss here\n","    # print(\"#\" + str(i) + \" Validation Loss: \" + str(val_loss))\n","\n","    # Early stopping\n","    if train_loss < min_val_loss:\n","        min_val_loss = train_loss\n","        wait = 0\n","        # Save the best model weights here\n","        NN.saveWeights(NN)\n","    else:\n","        wait += 1\n","        if wait >= patience:\n","            print(\"Early stopping\")\n","            break\n","\n","# Load the best model weights here\n","# NN.loadWeights()\n","\n","# Predict using normalized data\n","print(\"Predicted data based on trained weights: \")\n","print(\"Input (scaled): \\n\" + str(xPredicted_normalized))\n","print(\"Output: \\n\" + str(NN(xPredicted_normalized)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZtTa061aOPpe","executionInfo":{"status":"ok","timestamp":1690828577861,"user_tz":240,"elapsed":2740,"user":{"displayName":"Kai kleinbard","userId":"07244839972608712834"}},"outputId":"5579f503-c68a-4f0b-9548-37da9e74a716"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["#0 Train Loss: 0.9073054790496826\n","#1 Train Loss: 0.9073049426078796\n","#2 Train Loss: 0.9073043465614319\n","#3 Train Loss: 0.9073037505149841\n","#4 Train Loss: 0.9073031544685364\n","#5 Train Loss: 0.9073026180267334\n","#6 Train Loss: 0.9073019027709961\n","#7 Train Loss: 0.9073013663291931\n","#8 Train Loss: 0.9073007106781006\n","#9 Train Loss: 0.9073001742362976\n","#10 Train Loss: 0.9072995781898499\n","#11 Train Loss: 0.9072989821434021\n","#12 Train Loss: 0.9072983860969543\n","#13 Train Loss: 0.907297670841217\n","#14 Train Loss: 0.9072971940040588\n","#15 Train Loss: 0.9072966575622559\n","#16 Train Loss: 0.9072960019111633\n","#17 Train Loss: 0.9072954058647156\n","#18 Train Loss: 0.9072949290275574\n","#19 Train Loss: 0.9072942733764648\n","#20 Train Loss: 0.9072936177253723\n","#21 Train Loss: 0.9072930216789246\n","#22 Train Loss: 0.907292366027832\n","#23 Train Loss: 0.9072918891906738\n","#24 Train Loss: 0.9072912335395813\n","#25 Train Loss: 0.9072906374931335\n","#26 Train Loss: 0.9072900414466858\n","#27 Train Loss: 0.907289445400238\n","#28 Train Loss: 0.9072888493537903\n","#29 Train Loss: 0.9072882533073425\n","#30 Train Loss: 0.90728759765625\n","#31 Train Loss: 0.9072871208190918\n","#32 Train Loss: 0.9072864651679993\n","#33 Train Loss: 0.9072859287261963\n","#34 Train Loss: 0.9072852730751038\n","#35 Train Loss: 0.9072845578193665\n","#36 Train Loss: 0.9072840809822083\n","#37 Train Loss: 0.9072834849357605\n","#38 Train Loss: 0.9072828888893127\n","#39 Train Loss: 0.9072823524475098\n","#40 Train Loss: 0.9072816967964172\n","#41 Train Loss: 0.9072811007499695\n","#42 Train Loss: 0.9072805047035217\n","#43 Train Loss: 0.9072799682617188\n","#44 Train Loss: 0.9072793126106262\n","#45 Train Loss: 0.9072787165641785\n","#46 Train Loss: 0.9072781205177307\n","#47 Train Loss: 0.9072775840759277\n","#48 Train Loss: 0.9072769284248352\n","#49 Train Loss: 0.9072763919830322\n","#50 Train Loss: 0.9072756767272949\n","#51 Train Loss: 0.9072751402854919\n","#52 Train Loss: 0.9072745442390442\n","#53 Train Loss: 0.9072739481925964\n","#54 Train Loss: 0.9072733521461487\n","#55 Train Loss: 0.9072728157043457\n","#56 Train Loss: 0.9072721600532532\n","#57 Train Loss: 0.9072715640068054\n","#58 Train Loss: 0.9072709679603577\n","#59 Train Loss: 0.9072704315185547\n","#60 Train Loss: 0.9072698950767517\n","#61 Train Loss: 0.9072691798210144\n","#62 Train Loss: 0.9072685837745667\n","#63 Train Loss: 0.9072680473327637\n","#64 Train Loss: 0.9072673916816711\n","#65 Train Loss: 0.9072668552398682\n","#66 Train Loss: 0.9072661399841309\n","#67 Train Loss: 0.9072656631469727\n","#68 Train Loss: 0.9072650074958801\n","#69 Train Loss: 0.9072644114494324\n","#70 Train Loss: 0.9072638154029846\n","#71 Train Loss: 0.9072632789611816\n","#72 Train Loss: 0.9072626233100891\n","#73 Train Loss: 0.9072620868682861\n","#74 Train Loss: 0.9072613716125488\n","#75 Train Loss: 0.9072608947753906\n","#76 Train Loss: 0.9072603583335876\n","#77 Train Loss: 0.9072596430778503\n","#78 Train Loss: 0.9072590470314026\n","#79 Train Loss: 0.9072584509849548\n","#80 Train Loss: 0.9072579741477966\n","#81 Train Loss: 0.9072573184967041\n","#82 Train Loss: 0.9072566628456116\n","#83 Train Loss: 0.9072561264038086\n","#84 Train Loss: 0.9072555899620056\n","#85 Train Loss: 0.9072549343109131\n","#86 Train Loss: 0.9072543978691101\n","#87 Train Loss: 0.9072536826133728\n","#88 Train Loss: 0.907253086566925\n","#89 Train Loss: 0.9072525501251221\n","#90 Train Loss: 0.9072518944740295\n","#91 Train Loss: 0.9072513580322266\n","#92 Train Loss: 0.907250702381134\n","#93 Train Loss: 0.907250165939331\n","#94 Train Loss: 0.9072495102882385\n","#95 Train Loss: 0.9072489738464355\n","#96 Train Loss: 0.9072484374046326\n","#97 Train Loss: 0.9072478413581848\n","#98 Train Loss: 0.9072471261024475\n","#99 Train Loss: 0.9072465896606445\n","#100 Train Loss: 0.907245934009552\n","#101 Train Loss: 0.9072453379631042\n","#102 Train Loss: 0.907244861125946\n","#103 Train Loss: 0.9072442054748535\n","#104 Train Loss: 0.9072436690330505\n","#105 Train Loss: 0.907243013381958\n","#106 Train Loss: 0.907242476940155\n","#107 Train Loss: 0.9072418212890625\n","#108 Train Loss: 0.90724116563797\n","#109 Train Loss: 0.907240629196167\n","#110 Train Loss: 0.907240092754364\n","#111 Train Loss: 0.9072394371032715\n","#112 Train Loss: 0.9072389006614685\n","#113 Train Loss: 0.9072381854057312\n","#114 Train Loss: 0.9072375893592834\n","#115 Train Loss: 0.9072370529174805\n","#116 Train Loss: 0.9072363972663879\n","#117 Train Loss: 0.9072358012199402\n","#118 Train Loss: 0.9072352051734924\n","#119 Train Loss: 0.9072346687316895\n","#120 Train Loss: 0.9072341322898865\n","#121 Train Loss: 0.9072334170341492\n","#122 Train Loss: 0.907232940196991\n","#123 Train Loss: 0.9072322845458984\n","#124 Train Loss: 0.9072316288948059\n","#125 Train Loss: 0.9072310924530029\n","#126 Train Loss: 0.9072305560112\n","#127 Train Loss: 0.9072299003601074\n","#128 Train Loss: 0.9072293639183044\n","#129 Train Loss: 0.9072286486625671\n","#130 Train Loss: 0.9072281718254089\n","#131 Train Loss: 0.9072275161743164\n","#132 Train Loss: 0.9072268605232239\n","#133 Train Loss: 0.9072263240814209\n","#134 Train Loss: 0.9072256684303284\n","#135 Train Loss: 0.9072251319885254\n","#136 Train Loss: 0.9072245955467224\n","#137 Train Loss: 0.9072238802909851\n","#138 Train Loss: 0.9072234034538269\n","#139 Train Loss: 0.9072228074073792\n","#140 Train Loss: 0.9072222113609314\n","#141 Train Loss: 0.9072215557098389\n","#142 Train Loss: 0.9072209000587463\n","#143 Train Loss: 0.9072203636169434\n","#144 Train Loss: 0.9072198271751404\n","#145 Train Loss: 0.9072191119194031\n","#146 Train Loss: 0.9072186350822449\n","#147 Train Loss: 0.9072180390357971\n","#148 Train Loss: 0.9072173237800598\n","#149 Train Loss: 0.9072167873382568\n","#150 Train Loss: 0.9072162508964539\n","#151 Train Loss: 0.9072155952453613\n","#152 Train Loss: 0.9072150588035583\n","#153 Train Loss: 0.9072144031524658\n","#154 Train Loss: 0.9072138667106628\n","#155 Train Loss: 0.9072132110595703\n","#156 Train Loss: 0.9072125554084778\n","#157 Train Loss: 0.9072120189666748\n","#158 Train Loss: 0.9072113633155823\n","#159 Train Loss: 0.9072108268737793\n","#160 Train Loss: 0.9072102904319763\n","#161 Train Loss: 0.907209575176239\n","#162 Train Loss: 0.9072090983390808\n","#163 Train Loss: 0.9072084426879883\n","#164 Train Loss: 0.9072079062461853\n","#165 Train Loss: 0.9072073101997375\n","#166 Train Loss: 0.9072067141532898\n","#167 Train Loss: 0.9072060585021973\n","#168 Train Loss: 0.9072054028511047\n","#169 Train Loss: 0.9072048664093018\n","#170 Train Loss: 0.9072043299674988\n","#171 Train Loss: 0.9072036743164062\n","#172 Train Loss: 0.9072031378746033\n","#173 Train Loss: 0.9072025418281555\n","#174 Train Loss: 0.9072019457817078\n","#175 Train Loss: 0.9072012901306152\n","#176 Train Loss: 0.9072006344795227\n","#177 Train Loss: 0.9072000980377197\n","#178 Train Loss: 0.9071995615959167\n","#179 Train Loss: 0.9071989059448242\n","#180 Train Loss: 0.9071982502937317\n","#181 Train Loss: 0.9071977734565735\n","#182 Train Loss: 0.9071970582008362\n","#183 Train Loss: 0.9071965217590332\n","#184 Train Loss: 0.9071958661079407\n","#185 Train Loss: 0.9071953296661377\n","#186 Train Loss: 0.9071947932243347\n","#187 Train Loss: 0.9071941375732422\n","#188 Train Loss: 0.9071934819221497\n","#189 Train Loss: 0.9071930050849915\n","#190 Train Loss: 0.9071924090385437\n","#191 Train Loss: 0.907191812992096\n","#192 Train Loss: 0.9071910977363586\n","#193 Train Loss: 0.9071905612945557\n","#194 Train Loss: 0.9071900248527527\n","#195 Train Loss: 0.9071893692016602\n","#196 Train Loss: 0.9071888327598572\n","#197 Train Loss: 0.9071882367134094\n","#198 Train Loss: 0.9071876406669617\n","#199 Train Loss: 0.9071869850158691\n","#200 Train Loss: 0.9071863293647766\n","#201 Train Loss: 0.9071857929229736\n","#202 Train Loss: 0.9071852564811707\n","#203 Train Loss: 0.9071846008300781\n","#204 Train Loss: 0.9071840643882751\n","#205 Train Loss: 0.9071834087371826\n","#206 Train Loss: 0.9071828722953796\n","#207 Train Loss: 0.9071822762489319\n","#208 Train Loss: 0.9071815609931946\n","#209 Train Loss: 0.9071810245513916\n","#210 Train Loss: 0.9071804881095886\n","#211 Train Loss: 0.9071797728538513\n","#212 Train Loss: 0.9071792960166931\n","#213 Train Loss: 0.9071786403656006\n","#214 Train Loss: 0.9071781039237976\n","#215 Train Loss: 0.9071775078773499\n","#216 Train Loss: 0.9071769118309021\n","#217 Train Loss: 0.9071763157844543\n","#218 Train Loss: 0.9071757197380066\n","#219 Train Loss: 0.9071750640869141\n","#220 Train Loss: 0.9071745276451111\n","#221 Train Loss: 0.9071738719940186\n","#222 Train Loss: 0.9071733355522156\n","#223 Train Loss: 0.907172679901123\n","#224 Train Loss: 0.9071720242500305\n","#225 Train Loss: 0.9071715474128723\n","#226 Train Loss: 0.9071709513664246\n","#227 Train Loss: 0.907170295715332\n","#228 Train Loss: 0.907169759273529\n","#229 Train Loss: 0.9071691632270813\n","#230 Train Loss: 0.9071685671806335\n","#231 Train Loss: 0.9071679711341858\n","#232 Train Loss: 0.907167375087738\n","#233 Train Loss: 0.9071667790412903\n","#234 Train Loss: 0.9071661829948425\n","#235 Train Loss: 0.90716552734375\n","#236 Train Loss: 0.907164990901947\n","#237 Train Loss: 0.9071643352508545\n","#238 Train Loss: 0.9071637988090515\n","#239 Train Loss: 0.9071632027626038\n","#240 Train Loss: 0.907162606716156\n","#241 Train Loss: 0.9071620106697083\n","#242 Train Loss: 0.9071614146232605\n","#243 Train Loss: 0.907160758972168\n","#244 Train Loss: 0.907160222530365\n","#245 Train Loss: 0.9071596264839172\n","#246 Train Loss: 0.9071590304374695\n","#247 Train Loss: 0.9071584343910217\n","#248 Train Loss: 0.9071577191352844\n","#249 Train Loss: 0.9071571826934814\n","#250 Train Loss: 0.9071565270423889\n","#251 Train Loss: 0.9071559906005859\n","#252 Train Loss: 0.9071555137634277\n","#253 Train Loss: 0.9071548581123352\n","#254 Train Loss: 0.9071542620658875\n","#255 Train Loss: 0.9071536064147949\n","#256 Train Loss: 0.9071530699729919\n","#257 Train Loss: 0.9071524739265442\n","#258 Train Loss: 0.9071518778800964\n","#259 Train Loss: 0.9071512222290039\n","#260 Train Loss: 0.9071507453918457\n","#261 Train Loss: 0.9071500897407532\n","#262 Train Loss: 0.9071494936943054\n","#263 Train Loss: 0.9071488380432129\n","#264 Train Loss: 0.9071483016014099\n","#265 Train Loss: 0.9071477055549622\n","#266 Train Loss: 0.9071471095085144\n","#267 Train Loss: 0.9071464538574219\n","#268 Train Loss: 0.9071459174156189\n","#269 Train Loss: 0.9071452617645264\n","#270 Train Loss: 0.9071447253227234\n","#271 Train Loss: 0.9071440696716309\n","#272 Train Loss: 0.9071435928344727\n","#273 Train Loss: 0.9071429371833801\n","#274 Train Loss: 0.9071423411369324\n","#275 Train Loss: 0.9071416854858398\n","#276 Train Loss: 0.9071410298347473\n","#277 Train Loss: 0.9071404933929443\n","#278 Train Loss: 0.9071399569511414\n","#279 Train Loss: 0.9071393013000488\n","#280 Train Loss: 0.9071387648582458\n","#281 Train Loss: 0.9071381688117981\n","#282 Train Loss: 0.9071375727653503\n","#283 Train Loss: 0.9071369767189026\n","#284 Train Loss: 0.9071363806724548\n","#285 Train Loss: 0.9071357846260071\n","#286 Train Loss: 0.9071351885795593\n","#287 Train Loss: 0.9071345329284668\n","#288 Train Loss: 0.9071339964866638\n","#289 Train Loss: 0.9071334004402161\n","#290 Train Loss: 0.9071328043937683\n","#291 Train Loss: 0.9071322083473206\n","#292 Train Loss: 0.9071314930915833\n","#293 Train Loss: 0.9071309566497803\n","#294 Train Loss: 0.9071304202079773\n","#295 Train Loss: 0.9071298241615295\n","#296 Train Loss: 0.9071292281150818\n","#297 Train Loss: 0.9071285724639893\n","#298 Train Loss: 0.9071280360221863\n","#299 Train Loss: 0.9071275591850281\n","#300 Train Loss: 0.9071268439292908\n","#301 Train Loss: 0.907126247882843\n","#302 Train Loss: 0.9071256518363953\n","#303 Train Loss: 0.9071249961853027\n","#304 Train Loss: 0.9071244597434998\n","#305 Train Loss: 0.907123863697052\n","#306 Train Loss: 0.9071232676506042\n","#307 Train Loss: 0.9071226716041565\n","#308 Train Loss: 0.9071220755577087\n","#309 Train Loss: 0.907121479511261\n","#310 Train Loss: 0.9071208834648132\n","#311 Train Loss: 0.9071202874183655\n","#312 Train Loss: 0.9071196913719177\n","#313 Train Loss: 0.9071190357208252\n","#314 Train Loss: 0.9071184992790222\n","#315 Train Loss: 0.907118022441864\n","#316 Train Loss: 0.9071171879768372\n","#317 Train Loss: 0.907116711139679\n","#318 Train Loss: 0.9071161150932312\n","#319 Train Loss: 0.9071154594421387\n","#320 Train Loss: 0.9071149230003357\n","#321 Train Loss: 0.9071143269538879\n","#322 Train Loss: 0.907113790512085\n","#323 Train Loss: 0.9071131348609924\n","#324 Train Loss: 0.9071125388145447\n","#325 Train Loss: 0.9071119427680969\n","#326 Train Loss: 0.9071113467216492\n","#327 Train Loss: 0.9071107506752014\n","#328 Train Loss: 0.9071102142333984\n","#329 Train Loss: 0.9071094989776611\n","#330 Train Loss: 0.9071089625358582\n","#331 Train Loss: 0.9071083664894104\n","#332 Train Loss: 0.9071077704429626\n","#333 Train Loss: 0.9071071743965149\n","#334 Train Loss: 0.9071065783500671\n","#335 Train Loss: 0.9071059823036194\n","#336 Train Loss: 0.9071054458618164\n","#337 Train Loss: 0.9071047306060791\n","#338 Train Loss: 0.9071041941642761\n","#339 Train Loss: 0.9071035981178284\n","#340 Train Loss: 0.9071030020713806\n","#341 Train Loss: 0.9071024060249329\n","#342 Train Loss: 0.9071016907691956\n","#343 Train Loss: 0.9071011543273926\n","#344 Train Loss: 0.9071006178855896\n","#345 Train Loss: 0.9071000218391418\n","#346 Train Loss: 0.9070994257926941\n","#347 Train Loss: 0.9070987701416016\n","#348 Train Loss: 0.9070982933044434\n","#349 Train Loss: 0.9070976376533508\n","#350 Train Loss: 0.9070970416069031\n","#351 Train Loss: 0.9070964455604553\n","#352 Train Loss: 0.9070959091186523\n","#353 Train Loss: 0.9070952534675598\n","#354 Train Loss: 0.9070946574211121\n","#355 Train Loss: 0.9070940017700195\n","#356 Train Loss: 0.9070935249328613\n","#357 Train Loss: 0.9070928692817688\n","#358 Train Loss: 0.907092273235321\n","#359 Train Loss: 0.9070916175842285\n","#360 Train Loss: 0.9070910811424255\n","#361 Train Loss: 0.9070904850959778\n","#362 Train Loss: 0.90708988904953\n","#363 Train Loss: 0.9070892333984375\n","#364 Train Loss: 0.9070886969566345\n","#365 Train Loss: 0.9070882201194763\n","#366 Train Loss: 0.907087504863739\n","#367 Train Loss: 0.9070870280265808\n","#368 Train Loss: 0.9070863723754883\n","#369 Train Loss: 0.907085657119751\n","#370 Train Loss: 0.907085120677948\n","#371 Train Loss: 0.9070844650268555\n","#372 Train Loss: 0.9070839881896973\n","#373 Train Loss: 0.9070833325386047\n","#374 Train Loss: 0.907082736492157\n","#375 Train Loss: 0.9070821404457092\n","#376 Train Loss: 0.9070815443992615\n","#377 Train Loss: 0.9070809483528137\n","#378 Train Loss: 0.9070804119110107\n","#379 Train Loss: 0.9070796966552734\n","#380 Train Loss: 0.9070792198181152\n","#381 Train Loss: 0.9070785641670227\n","#382 Train Loss: 0.907077968120575\n","#383 Train Loss: 0.9070773720741272\n","#384 Train Loss: 0.9070767760276794\n","#385 Train Loss: 0.9070761799812317\n","#386 Train Loss: 0.9070755839347839\n","#387 Train Loss: 0.9070749878883362\n","#388 Train Loss: 0.9070744514465332\n","#389 Train Loss: 0.9070737957954407\n","#390 Train Loss: 0.9070731997489929\n","#391 Train Loss: 0.9070726037025452\n","#392 Train Loss: 0.9070720672607422\n","#393 Train Loss: 0.9070714116096497\n","#394 Train Loss: 0.9070708751678467\n","#395 Train Loss: 0.9070702195167542\n","#396 Train Loss: 0.9070696234703064\n","#397 Train Loss: 0.9070689678192139\n","#398 Train Loss: 0.9070684313774109\n","#399 Train Loss: 0.9070678353309631\n","#400 Train Loss: 0.9070672392845154\n","#401 Train Loss: 0.9070666432380676\n","#402 Train Loss: 0.9070661067962646\n","#403 Train Loss: 0.9070653915405273\n","#404 Train Loss: 0.9070648550987244\n","#405 Train Loss: 0.9070642590522766\n","#406 Train Loss: 0.9070637226104736\n","#407 Train Loss: 0.9070631861686707\n","#408 Train Loss: 0.9070624709129333\n","#409 Train Loss: 0.9070618748664856\n","#410 Train Loss: 0.9070612788200378\n","#411 Train Loss: 0.9070606827735901\n","#412 Train Loss: 0.9070600867271423\n","#413 Train Loss: 0.9070594310760498\n","#414 Train Loss: 0.9070588946342468\n","#415 Train Loss: 0.9070584177970886\n","#416 Train Loss: 0.9070577025413513\n","#417 Train Loss: 0.9070571064949036\n","#418 Train Loss: 0.9070565104484558\n","#419 Train Loss: 0.9070559144020081\n","#420 Train Loss: 0.9070553779602051\n","#421 Train Loss: 0.9070547223091125\n","#422 Train Loss: 0.9070541858673096\n","#423 Train Loss: 0.907053530216217\n","#424 Train Loss: 0.9070529341697693\n","#425 Train Loss: 0.9070523381233215\n","#426 Train Loss: 0.9070517420768738\n","#427 Train Loss: 0.907051146030426\n","#428 Train Loss: 0.907050609588623\n","#429 Train Loss: 0.9070499539375305\n","#430 Train Loss: 0.9070494174957275\n","#431 Train Loss: 0.907048761844635\n","#432 Train Loss: 0.907048225402832\n","#433 Train Loss: 0.9070475697517395\n","#434 Train Loss: 0.9070469737052917\n","#435 Train Loss: 0.907046377658844\n","#436 Train Loss: 0.907045841217041\n","#437 Train Loss: 0.9070451855659485\n","#438 Train Loss: 0.9070446491241455\n","#439 Train Loss: 0.9070439338684082\n","#440 Train Loss: 0.9070433974266052\n","#441 Train Loss: 0.9070428013801575\n","#442 Train Loss: 0.9070422053337097\n","#443 Train Loss: 0.907041609287262\n","#444 Train Loss: 0.907041072845459\n","#445 Train Loss: 0.9070404171943665\n","#446 Train Loss: 0.9070398807525635\n","#447 Train Loss: 0.9070391654968262\n","#448 Train Loss: 0.9070386290550232\n","#449 Train Loss: 0.907038152217865\n","#450 Train Loss: 0.9070374369621277\n","#451 Train Loss: 0.9070368409156799\n","#452 Train Loss: 0.907036304473877\n","#453 Train Loss: 0.9070356488227844\n","#454 Train Loss: 0.9070350527763367\n","#455 Train Loss: 0.9070343971252441\n","#456 Train Loss: 0.9070339202880859\n","#457 Train Loss: 0.907033383846283\n","#458 Train Loss: 0.9070326685905457\n","#459 Train Loss: 0.9070320725440979\n","#460 Train Loss: 0.9070315361022949\n","#461 Train Loss: 0.9070308804512024\n","#462 Train Loss: 0.9070303440093994\n","#463 Train Loss: 0.9070296287536621\n","#464 Train Loss: 0.9070291519165039\n","#465 Train Loss: 0.9070284962654114\n","#466 Train Loss: 0.9070279002189636\n","#467 Train Loss: 0.9070274233818054\n","#468 Train Loss: 0.9070267081260681\n","#469 Train Loss: 0.9070261120796204\n","#470 Train Loss: 0.9070255756378174\n","#471 Train Loss: 0.9070249199867249\n","#472 Train Loss: 0.9070243835449219\n","#473 Train Loss: 0.9070237278938293\n","#474 Train Loss: 0.9070231318473816\n","#475 Train Loss: 0.9070226550102234\n","#476 Train Loss: 0.9070219993591309\n","#477 Train Loss: 0.9070213437080383\n","#478 Train Loss: 0.9070207476615906\n","#479 Train Loss: 0.9070201516151428\n","#480 Train Loss: 0.9070196151733398\n","#481 Train Loss: 0.9070189595222473\n","#482 Train Loss: 0.9070183634757996\n","#483 Train Loss: 0.9070178866386414\n","#484 Train Loss: 0.907017171382904\n","#485 Train Loss: 0.9070165753364563\n","#486 Train Loss: 0.9070160388946533\n","#487 Train Loss: 0.9070153832435608\n","#488 Train Loss: 0.9070148468017578\n","#489 Train Loss: 0.9070141911506653\n","#490 Train Loss: 0.9070136547088623\n","#491 Train Loss: 0.9070131182670593\n","#492 Train Loss: 0.9070122838020325\n","#493 Train Loss: 0.9070118069648743\n","#494 Train Loss: 0.9070112705230713\n","#495 Train Loss: 0.9070106148719788\n","#496 Train Loss: 0.9070100784301758\n","#497 Train Loss: 0.9070094227790833\n","#498 Train Loss: 0.9070088267326355\n","#499 Train Loss: 0.9070083498954773\n","#500 Train Loss: 0.90700763463974\n","#501 Train Loss: 0.9070070385932922\n","#502 Train Loss: 0.907006561756134\n","#503 Train Loss: 0.9070058465003967\n","#504 Train Loss: 0.907005250453949\n","#505 Train Loss: 0.9070045948028564\n","#506 Train Loss: 0.9070041179656982\n","#507 Train Loss: 0.9070034623146057\n","#508 Train Loss: 0.907002866268158\n","#509 Train Loss: 0.9070022702217102\n","#510 Train Loss: 0.9070017337799072\n","#511 Train Loss: 0.9070010781288147\n","#512 Train Loss: 0.9070005416870117\n","#513 Train Loss: 0.9069998860359192\n","#514 Train Loss: 0.9069992899894714\n","#515 Train Loss: 0.9069988131523132\n","#516 Train Loss: 0.9069981575012207\n","#517 Train Loss: 0.9069975018501282\n","#518 Train Loss: 0.9069969058036804\n","#519 Train Loss: 0.9069963097572327\n","#520 Train Loss: 0.9069957733154297\n","#521 Train Loss: 0.9069951176643372\n","#522 Train Loss: 0.9069945216178894\n","#523 Train Loss: 0.9069940447807312\n","#524 Train Loss: 0.9069933295249939\n","#525 Train Loss: 0.9069927334785461\n","#526 Train Loss: 0.9069921374320984\n","#527 Train Loss: 0.9069915413856506\n","#528 Train Loss: 0.9069910645484924\n","#529 Train Loss: 0.9069903492927551\n","#530 Train Loss: 0.9069898128509521\n","#531 Train Loss: 0.9069891571998596\n","#532 Train Loss: 0.9069886207580566\n","#533 Train Loss: 0.9069880843162537\n","#534 Train Loss: 0.9069873690605164\n","#535 Train Loss: 0.9069867730140686\n","#536 Train Loss: 0.9069862365722656\n","#537 Train Loss: 0.9069855809211731\n","#538 Train Loss: 0.9069850444793701\n","#539 Train Loss: 0.9069843888282776\n","#540 Train Loss: 0.9069838523864746\n","#541 Train Loss: 0.9069833159446716\n","#542 Train Loss: 0.9069826006889343\n","#543 Train Loss: 0.9069820046424866\n","#544 Train Loss: 0.9069814682006836\n","#545 Train Loss: 0.9069808125495911\n","#546 Train Loss: 0.9069802761077881\n","#547 Train Loss: 0.9069795608520508\n","#548 Train Loss: 0.9069790244102478\n","#549 Train Loss: 0.9069784283638\n","#550 Train Loss: 0.9069778323173523\n","#551 Train Loss: 0.9069773554801941\n","#552 Train Loss: 0.9069767594337463\n","#553 Train Loss: 0.906976044178009\n","#554 Train Loss: 0.9069755673408508\n","#555 Train Loss: 0.9069748520851135\n","#556 Train Loss: 0.9069743156433105\n","#557 Train Loss: 0.9069737792015076\n","#558 Train Loss: 0.9069730639457703\n","#559 Train Loss: 0.9069725871086121\n","#560 Train Loss: 0.9069719314575195\n","#561 Train Loss: 0.906971275806427\n","#562 Train Loss: 0.906970739364624\n","#563 Train Loss: 0.9069700837135315\n","#564 Train Loss: 0.9069695472717285\n","#565 Train Loss: 0.906968891620636\n","#566 Train Loss: 0.9069682955741882\n","#567 Train Loss: 0.90696781873703\n","#568 Train Loss: 0.9069671034812927\n","#569 Train Loss: 0.906966507434845\n","#570 Train Loss: 0.906965970993042\n","#571 Train Loss: 0.9069652557373047\n","#572 Train Loss: 0.9069647789001465\n","#573 Train Loss: 0.906964123249054\n","#574 Train Loss: 0.9069635272026062\n","#575 Train Loss: 0.9069629311561584\n","#576 Train Loss: 0.9069623351097107\n","#577 Train Loss: 0.9069617390632629\n","#578 Train Loss: 0.9069612622261047\n","#579 Train Loss: 0.9069605469703674\n","#580 Train Loss: 0.9069600105285645\n","#581 Train Loss: 0.9069593548774719\n","#582 Train Loss: 0.906958818435669\n","#583 Train Loss: 0.906958281993866\n","#584 Train Loss: 0.9069576263427734\n","#585 Train Loss: 0.9069569706916809\n","#586 Train Loss: 0.9069564342498779\n","#587 Train Loss: 0.9069557189941406\n","#588 Train Loss: 0.9069552421569824\n","#589 Train Loss: 0.9069545865058899\n","#590 Train Loss: 0.9069540500640869\n","#591 Train Loss: 0.9069535136222839\n","#592 Train Loss: 0.9069527983665466\n","#593 Train Loss: 0.9069522023200989\n","#594 Train Loss: 0.9069517254829407\n","#595 Train Loss: 0.9069510102272034\n","#596 Train Loss: 0.9069504737854004\n","#597 Train Loss: 0.9069498181343079\n","#598 Train Loss: 0.9069492220878601\n","#599 Train Loss: 0.9069486260414124\n","#600 Train Loss: 0.9069480299949646\n","#601 Train Loss: 0.9069474339485168\n","#602 Train Loss: 0.9069468975067139\n","#603 Train Loss: 0.9069462418556213\n","#604 Train Loss: 0.9069457054138184\n","#605 Train Loss: 0.9069450497627258\n","#606 Train Loss: 0.9069444537162781\n","#607 Train Loss: 0.9069439768791199\n","#608 Train Loss: 0.9069433212280273\n","#609 Train Loss: 0.9069426655769348\n","#610 Train Loss: 0.9069420695304871\n","#611 Train Loss: 0.9069415926933289\n","#612 Train Loss: 0.9069409370422363\n","#613 Train Loss: 0.9069402813911438\n","#614 Train Loss: 0.9069397449493408\n","#615 Train Loss: 0.9069390892982483\n","#616 Train Loss: 0.9069385528564453\n","#617 Train Loss: 0.9069378972053528\n","#618 Train Loss: 0.906937301158905\n","#619 Train Loss: 0.9069367051124573\n","#620 Train Loss: 0.9069361686706543\n","#621 Train Loss: 0.9069355130195618\n","#622 Train Loss: 0.9069349765777588\n","#623 Train Loss: 0.9069344401359558\n","#624 Train Loss: 0.9069337844848633\n","#625 Train Loss: 0.9069332480430603\n","#626 Train Loss: 0.906932532787323\n","#627 Train Loss: 0.9069319367408752\n","#628 Train Loss: 0.9069313406944275\n","#629 Train Loss: 0.9069307446479797\n","#630 Train Loss: 0.9069302082061768\n","#631 Train Loss: 0.9069295525550842\n","#632 Train Loss: 0.9069289565086365\n","#633 Train Loss: 0.9069284796714783\n","#634 Train Loss: 0.9069278240203857\n","#635 Train Loss: 0.9069271683692932\n","#636 Train Loss: 0.9069266319274902\n","#637 Train Loss: 0.9069259762763977\n","#638 Train Loss: 0.9069254398345947\n","#639 Train Loss: 0.9069247841835022\n","#640 Train Loss: 0.9069242477416992\n","#641 Train Loss: 0.9069235920906067\n","#642 Train Loss: 0.9069229960441589\n","#643 Train Loss: 0.9069223999977112\n","#644 Train Loss: 0.9069218635559082\n","#645 Train Loss: 0.9069212079048157\n","#646 Train Loss: 0.9069206714630127\n","#647 Train Loss: 0.9069199562072754\n","#648 Train Loss: 0.9069194793701172\n","#649 Train Loss: 0.9069189429283142\n","#650 Train Loss: 0.9069182872772217\n","#651 Train Loss: 0.9069177508354187\n","#652 Train Loss: 0.9069170355796814\n","#653 Train Loss: 0.9069164395332336\n","#654 Train Loss: 0.9069158434867859\n","#655 Train Loss: 0.9069152474403381\n","#656 Train Loss: 0.9069147109985352\n","#657 Train Loss: 0.9069141745567322\n","#658 Train Loss: 0.9069134593009949\n","#659 Train Loss: 0.9069128632545471\n","#660 Train Loss: 0.9069123268127441\n","#661 Train Loss: 0.9069116711616516\n","#662 Train Loss: 0.9069111347198486\n","#663 Train Loss: 0.9069104790687561\n","#664 Train Loss: 0.9069099426269531\n","#665 Train Loss: 0.9069094061851501\n","#666 Train Loss: 0.9069086909294128\n","#667 Train Loss: 0.9069080948829651\n","#668 Train Loss: 0.9069074988365173\n","#669 Train Loss: 0.9069069027900696\n","#670 Train Loss: 0.9069063663482666\n","#671 Train Loss: 0.9069056510925293\n","#672 Train Loss: 0.9069051742553711\n","#673 Train Loss: 0.9069045186042786\n","#674 Train Loss: 0.9069039821624756\n","#675 Train Loss: 0.9069034457206726\n","#676 Train Loss: 0.9069027900695801\n","#677 Train Loss: 0.9069022536277771\n","#678 Train Loss: 0.9069016575813293\n","#679 Train Loss: 0.906900942325592\n","#680 Train Loss: 0.9069003462791443\n","#681 Train Loss: 0.9068997502326965\n","#682 Train Loss: 0.9068992137908936\n","#683 Train Loss: 0.9068986773490906\n","#684 Train Loss: 0.9068979620933533\n","#685 Train Loss: 0.9068973660469055\n","#686 Train Loss: 0.9068968296051025\n","#687 Train Loss: 0.90689617395401\n","#688 Train Loss: 0.906895637512207\n","#689 Train Loss: 0.9068949818611145\n","#690 Train Loss: 0.9068944454193115\n","#691 Train Loss: 0.9068939089775085\n","#692 Train Loss: 0.9068931937217712\n","#693 Train Loss: 0.9068925976753235\n","#694 Train Loss: 0.9068920612335205\n","#695 Train Loss: 0.906891405582428\n","#696 Train Loss: 0.906890869140625\n","#697 Train Loss: 0.9068901538848877\n","#698 Train Loss: 0.9068896174430847\n","#699 Train Loss: 0.906889021396637\n","#700 Train Loss: 0.906888484954834\n","#701 Train Loss: 0.906887948513031\n","#702 Train Loss: 0.9068872332572937\n","#703 Train Loss: 0.906886637210846\n","#704 Train Loss: 0.906886100769043\n","#705 Train Loss: 0.9068854451179504\n","#706 Train Loss: 0.9068849086761475\n","#707 Train Loss: 0.9068842530250549\n","#708 Train Loss: 0.906883716583252\n","#709 Train Loss: 0.9068830609321594\n","#710 Train Loss: 0.9068824648857117\n","#711 Train Loss: 0.9068818688392639\n","#712 Train Loss: 0.9068813323974609\n","#713 Train Loss: 0.9068806767463684\n","#714 Train Loss: 0.9068801403045654\n","#715 Train Loss: 0.9068796038627625\n","#716 Train Loss: 0.9068788886070251\n","#717 Train Loss: 0.9068782925605774\n","#718 Train Loss: 0.9068776965141296\n","#719 Train Loss: 0.9068771004676819\n","#720 Train Loss: 0.9068765640258789\n","#721 Train Loss: 0.9068759083747864\n","#722 Train Loss: 0.9068753123283386\n","#723 Train Loss: 0.9068746566772461\n","#724 Train Loss: 0.9068741798400879\n","#725 Train Loss: 0.9068735241889954\n","#726 Train Loss: 0.9068729281425476\n","#727 Train Loss: 0.9068724513053894\n","#728 Train Loss: 0.9068717956542969\n","#729 Train Loss: 0.9068711400032043\n","#730 Train Loss: 0.9068706035614014\n","#731 Train Loss: 0.9068699479103088\n","#732 Train Loss: 0.9068694114685059\n","#733 Train Loss: 0.9068687558174133\n","#734 Train Loss: 0.9068682193756104\n","#735 Train Loss: 0.9068676829338074\n","#736 Train Loss: 0.9068670272827148\n","#737 Train Loss: 0.9068663716316223\n","#738 Train Loss: 0.9068658351898193\n","#739 Train Loss: 0.9068651795387268\n","#740 Train Loss: 0.9068646430969238\n","#741 Train Loss: 0.9068639874458313\n","#742 Train Loss: 0.9068633913993835\n","#743 Train Loss: 0.9068627953529358\n","#744 Train Loss: 0.9068622589111328\n","#745 Train Loss: 0.9068616032600403\n","#746 Train Loss: 0.9068611264228821\n","#747 Train Loss: 0.9068603515625\n","#748 Train Loss: 0.906859815120697\n","#749 Train Loss: 0.9068592190742493\n","#750 Train Loss: 0.9068586230278015\n","#751 Train Loss: 0.906857967376709\n","#752 Train Loss: 0.906857430934906\n","#753 Train Loss: 0.9068568348884583\n","#754 Train Loss: 0.9068562984466553\n","#755 Train Loss: 0.906855583190918\n","#756 Train Loss: 0.9068551063537598\n","#757 Train Loss: 0.9068544507026672\n","#758 Train Loss: 0.9068539142608643\n","#759 Train Loss: 0.9068532586097717\n","#760 Train Loss: 0.906852662563324\n","#761 Train Loss: 0.9068520665168762\n","#762 Train Loss: 0.9068514704704285\n","#763 Train Loss: 0.9068508744239807\n","#764 Train Loss: 0.9068503379821777\n","#765 Train Loss: 0.9068498015403748\n","#766 Train Loss: 0.9068490862846375\n","#767 Train Loss: 0.9068484902381897\n","#768 Train Loss: 0.9068478941917419\n","#769 Train Loss: 0.9068472981452942\n","#770 Train Loss: 0.9068467617034912\n","#771 Train Loss: 0.9068461060523987\n","#772 Train Loss: 0.9068455100059509\n","#773 Train Loss: 0.9068449139595032\n","#774 Train Loss: 0.9068443179130554\n","#775 Train Loss: 0.9068437218666077\n","#776 Train Loss: 0.9068431258201599\n","#777 Train Loss: 0.9068425297737122\n","#778 Train Loss: 0.9068419933319092\n","#779 Train Loss: 0.9068412780761719\n","#780 Train Loss: 0.9068408012390137\n","#781 Train Loss: 0.9068400859832764\n","#782 Train Loss: 0.9068396091461182\n","#783 Train Loss: 0.9068389534950256\n","#784 Train Loss: 0.9068384170532227\n","#785 Train Loss: 0.9068377614021301\n","#786 Train Loss: 0.9068372249603271\n","#787 Train Loss: 0.9068365693092346\n","#788 Train Loss: 0.9068360328674316\n","#789 Train Loss: 0.9068353772163391\n","#790 Train Loss: 0.9068347811698914\n","#791 Train Loss: 0.9068343043327332\n","#792 Train Loss: 0.9068335890769958\n","#793 Train Loss: 0.9068329930305481\n","#794 Train Loss: 0.9068324565887451\n","#795 Train Loss: 0.9068317413330078\n","#796 Train Loss: 0.9068312644958496\n","#797 Train Loss: 0.9068306088447571\n","#798 Train Loss: 0.9068300127983093\n","#799 Train Loss: 0.9068295359611511\n","#800 Train Loss: 0.9068288803100586\n","#801 Train Loss: 0.9068282246589661\n","#802 Train Loss: 0.9068276286125183\n","#803 Train Loss: 0.9068269729614258\n","#804 Train Loss: 0.9068264961242676\n","#805 Train Loss: 0.9068257808685303\n","#806 Train Loss: 0.9068252444267273\n","#807 Train Loss: 0.9068246483802795\n","#808 Train Loss: 0.9068239331245422\n","#809 Train Loss: 0.906823456287384\n","#810 Train Loss: 0.9068228602409363\n","#811 Train Loss: 0.9068222641944885\n","#812 Train Loss: 0.9068217277526855\n","#813 Train Loss: 0.906821072101593\n","#814 Train Loss: 0.90682053565979\n","#815 Train Loss: 0.9068199992179871\n","#816 Train Loss: 0.9068192839622498\n","#817 Train Loss: 0.906818687915802\n","#818 Train Loss: 0.9068180918693542\n","#819 Train Loss: 0.9068174958229065\n","#820 Train Loss: 0.9068169593811035\n","#821 Train Loss: 0.9068162441253662\n","#822 Train Loss: 0.9068157076835632\n","#823 Train Loss: 0.9068150520324707\n","#824 Train Loss: 0.9068145155906677\n","#825 Train Loss: 0.90681391954422\n","#826 Train Loss: 0.9068133234977722\n","#827 Train Loss: 0.9068127274513245\n","#828 Train Loss: 0.9068121910095215\n","#829 Train Loss: 0.9068114757537842\n","#830 Train Loss: 0.9068109393119812\n","#831 Train Loss: 0.9068102836608887\n","#832 Train Loss: 0.9068097472190857\n","#833 Train Loss: 0.9068091511726379\n","#834 Train Loss: 0.9068084359169006\n","#835 Train Loss: 0.9068079590797424\n","#836 Train Loss: 0.9068074226379395\n","#837 Train Loss: 0.9068067669868469\n","#838 Train Loss: 0.906806230545044\n","#839 Train Loss: 0.9068055152893066\n","#840 Train Loss: 0.9068050384521484\n","#841 Train Loss: 0.9068043828010559\n","#842 Train Loss: 0.9068037867546082\n","#843 Train Loss: 0.9068031907081604\n","#844 Train Loss: 0.9068025946617126\n","#845 Train Loss: 0.9068019986152649\n","#846 Train Loss: 0.9068014025688171\n","#847 Train Loss: 0.9068007469177246\n","#848 Train Loss: 0.9068002700805664\n","#849 Train Loss: 0.9067996144294739\n","#850 Train Loss: 0.9067990183830261\n","#851 Train Loss: 0.9067984223365784\n","#852 Train Loss: 0.9067977070808411\n","#853 Train Loss: 0.9067972302436829\n","#854 Train Loss: 0.9067966938018799\n","#855 Train Loss: 0.9067959785461426\n","#856 Train Loss: 0.9067954421043396\n","#857 Train Loss: 0.9067948460578918\n","#858 Train Loss: 0.9067942500114441\n","#859 Train Loss: 0.9067935943603516\n","#860 Train Loss: 0.906792938709259\n","#861 Train Loss: 0.9067924618721008\n","#862 Train Loss: 0.9067919254302979\n","#863 Train Loss: 0.9067912101745605\n","#864 Train Loss: 0.9067907333374023\n","#865 Train Loss: 0.906790018081665\n","#866 Train Loss: 0.9067894816398621\n","#867 Train Loss: 0.9067888855934143\n","#868 Train Loss: 0.9067882895469666\n","#869 Train Loss: 0.9067876935005188\n","#870 Train Loss: 0.906787097454071\n","#871 Train Loss: 0.9067865014076233\n","#872 Train Loss: 0.9067859053611755\n","#873 Train Loss: 0.906785249710083\n","#874 Train Loss: 0.90678471326828\n","#875 Train Loss: 0.9067841172218323\n","#876 Train Loss: 0.9067835211753845\n","#877 Train Loss: 0.9067829251289368\n","#878 Train Loss: 0.906782329082489\n","#879 Train Loss: 0.9067816734313965\n","#880 Train Loss: 0.9067811965942383\n","#881 Train Loss: 0.906780481338501\n","#882 Train Loss: 0.906779944896698\n","#883 Train Loss: 0.9067793488502502\n","#884 Train Loss: 0.9067786335945129\n","#885 Train Loss: 0.90677809715271\n","#886 Train Loss: 0.906777560710907\n","#887 Train Loss: 0.9067769050598145\n","#888 Train Loss: 0.9067763686180115\n","#889 Train Loss: 0.906775712966919\n","#890 Train Loss: 0.906775176525116\n","#891 Train Loss: 0.9067745804786682\n","#892 Train Loss: 0.9067739844322205\n","#893 Train Loss: 0.9067733883857727\n","#894 Train Loss: 0.906772792339325\n","#895 Train Loss: 0.9067721366882324\n","#896 Train Loss: 0.9067716002464294\n","#897 Train Loss: 0.9067710041999817\n","#898 Train Loss: 0.9067704081535339\n","#899 Train Loss: 0.9067698121070862\n","#900 Train Loss: 0.9067692160606384\n","#901 Train Loss: 0.9067686200141907\n","#902 Train Loss: 0.9067680239677429\n","#903 Train Loss: 0.9067673683166504\n","#904 Train Loss: 0.9067668318748474\n","#905 Train Loss: 0.9067662358283997\n","#906 Train Loss: 0.9067656397819519\n","#907 Train Loss: 0.9067650437355042\n","#908 Train Loss: 0.9067644476890564\n","#909 Train Loss: 0.9067638516426086\n","#910 Train Loss: 0.9067632555961609\n","#911 Train Loss: 0.9067625999450684\n","#912 Train Loss: 0.9067620635032654\n","#913 Train Loss: 0.9067614078521729\n","#914 Train Loss: 0.9067608714103699\n","#915 Train Loss: 0.9067602157592773\n","#916 Train Loss: 0.9067596793174744\n","#917 Train Loss: 0.9067590832710266\n","#918 Train Loss: 0.9067584872245789\n","#919 Train Loss: 0.9067578911781311\n","#920 Train Loss: 0.9067572951316833\n","#921 Train Loss: 0.9067566990852356\n","#922 Train Loss: 0.9067561030387878\n","#923 Train Loss: 0.9067554473876953\n","#924 Train Loss: 0.9067547917366028\n","#925 Train Loss: 0.9067543148994446\n","#926 Train Loss: 0.9067537188529968\n","#927 Train Loss: 0.9067531228065491\n","#928 Train Loss: 0.9067525863647461\n","#929 Train Loss: 0.9067518711090088\n","#930 Train Loss: 0.9067513346672058\n","#931 Train Loss: 0.9067506790161133\n","#932 Train Loss: 0.9067501425743103\n","#933 Train Loss: 0.9067495465278625\n","#934 Train Loss: 0.9067489504814148\n","#935 Train Loss: 0.9067482948303223\n","#936 Train Loss: 0.9067478179931641\n","#937 Train Loss: 0.9067471027374268\n","#938 Train Loss: 0.9067465662956238\n","#939 Train Loss: 0.9067459106445312\n","#940 Train Loss: 0.9067452549934387\n","#941 Train Loss: 0.9067447781562805\n","#942 Train Loss: 0.9067440629005432\n","#943 Train Loss: 0.9067435264587402\n","#944 Train Loss: 0.9067429900169373\n","#945 Train Loss: 0.9067423939704895\n","#946 Train Loss: 0.9067417979240417\n","#947 Train Loss: 0.9067411422729492\n","#948 Train Loss: 0.9067406058311462\n","#949 Train Loss: 0.9067400097846985\n","#950 Train Loss: 0.9067394137382507\n","#951 Train Loss: 0.906738817691803\n","#952 Train Loss: 0.9067381024360657\n","#953 Train Loss: 0.9067375659942627\n","#954 Train Loss: 0.9067370295524597\n","#955 Train Loss: 0.9067363739013672\n","#956 Train Loss: 0.9067358374595642\n","#957 Train Loss: 0.9067352414131165\n","#958 Train Loss: 0.9067346453666687\n","#959 Train Loss: 0.9067339897155762\n","#960 Train Loss: 0.9067333340644836\n","#961 Train Loss: 0.9067327976226807\n","#962 Train Loss: 0.9067322611808777\n","#963 Train Loss: 0.9067316651344299\n","#964 Train Loss: 0.9067310690879822\n","#965 Train Loss: 0.9067303538322449\n","#966 Train Loss: 0.9067297577857971\n","#967 Train Loss: 0.9067292213439941\n","#968 Train Loss: 0.9067285656929016\n","#969 Train Loss: 0.9067280292510986\n","#970 Train Loss: 0.9067274928092957\n","#971 Train Loss: 0.9067268371582031\n","#972 Train Loss: 0.9067263007164001\n","#973 Train Loss: 0.9067256450653076\n","#974 Train Loss: 0.9067251086235046\n","#975 Train Loss: 0.9067245125770569\n","#976 Train Loss: 0.9067239165306091\n","#977 Train Loss: 0.9067233204841614\n","#978 Train Loss: 0.9067227244377136\n","#979 Train Loss: 0.9067220687866211\n","#980 Train Loss: 0.9067215323448181\n","#981 Train Loss: 0.9067208766937256\n","#982 Train Loss: 0.9067203402519226\n","#983 Train Loss: 0.9067196846008301\n","#984 Train Loss: 0.9067191481590271\n","#985 Train Loss: 0.9067185521125793\n","#986 Train Loss: 0.906717836856842\n","#987 Train Loss: 0.9067173004150391\n","#988 Train Loss: 0.9067167639732361\n","#989 Train Loss: 0.9067161083221436\n","#990 Train Loss: 0.9067155718803406\n","#991 Train Loss: 0.9067149758338928\n","#992 Train Loss: 0.9067142605781555\n","#993 Train Loss: 0.9067137241363525\n","#994 Train Loss: 0.90671306848526\n","#995 Train Loss: 0.906712532043457\n","#996 Train Loss: 0.9067118763923645\n","#997 Train Loss: 0.9067113399505615\n","#998 Train Loss: 0.9067108035087585\n","#999 Train Loss: 0.9067102074623108\n","Predicted data based on trained weights: \n","Input (scaled): \n","tensor([[-0.5000,  1.1209]])\n","Output: \n","tensor([[-0.0127]])\n"]}]}]}